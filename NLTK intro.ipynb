{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e811798",
   "metadata": {},
   "source": [
    "# Import NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe28ca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62234606",
   "metadata": {},
   "source": [
    "# Install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c93e7c3",
   "metadata": {},
   "source": [
    "Wordcloud is basically a visualization technique to represent the frequency of words in a text where the size of the word represents its frequency. In order to work with wordclouds in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96b6a436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -lick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ick (c:\\programdata\\anaconda3\\lib\\site-packages)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in c:\\programdata\\anaconda3\\lib\\site-packages (1.8.2.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (3.4.3)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (3.0.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Ignoring invalid distribution -lick (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0331f",
   "metadata": {},
   "source": [
    "# Import Word tokenizer and sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c95e8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5709ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"Pakistan is my country. I love pakistan. It has four seasons.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649abb16",
   "metadata": {},
   "source": [
    "Word tokenization is the process of splitting a large sample of text into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0247cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1533c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pakistan',\n",
       " 'is',\n",
       " 'my',\n",
       " 'country',\n",
       " '.',\n",
       " 'I',\n",
       " 'love',\n",
       " 'pakistan',\n",
       " '.',\n",
       " 'It',\n",
       " 'has',\n",
       " 'four',\n",
       " 'seasons',\n",
       " '.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa49808a",
   "metadata": {},
   "source": [
    "Sentence tokenization is the process of splitting text into individual sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c40d557a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pakistan is my country.', 'I love pakistan.', 'It has four seasons.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_token = sent_tokenize(sent)\n",
    "sent_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f86c02",
   "metadata": {},
   "source": [
    "# import FerqDist "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de0f854",
   "metadata": {},
   "source": [
    "FreqDist class is used to encode “frequency distributions”, which count the number of times that each outcome of an experiment occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "95d2350e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('.', 3), ('Pakistan', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "f= FreqDist(words)\n",
    "f.most_common(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae660530",
   "metadata": {},
   "source": [
    "To convert the words into lower case and separate the punctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a54325bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list=[]\n",
    "pun=[]\n",
    "for w in words:\n",
    "    if w.isalpha():\n",
    "        word_list.append(w.lower())\n",
    "    else:\n",
    "        pun.append(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "268cef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pakistan', 'is', 'my', 'country', 'i', 'love', 'pakistan', 'it', 'has', 'four', 'seasons']\n",
      "['.', '.', '.']\n"
     ]
    }
   ],
   "source": [
    "print(word_list)\n",
    "print(pun)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df6428",
   "metadata": {},
   "source": [
    "# Import Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7293740",
   "metadata": {},
   "source": [
    "Stop words are words that are so common they are basically ignored by typical tokenizers. By default, NLTK (Natural Language Toolkit) includes a list of 40 stop words, including: “a”, “an”, “the”, “of”, “in”, etc. The stopwords in nltk are the most common words in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47490db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_word=stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc977b9",
   "metadata": {},
   "source": [
    "To remove stopwords from text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e60de2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['Pakistan', 'is', 'my', 'country', '.', 'I', 'love', 'pakistan']\n",
    "word_list=[]\n",
    "without_stopwords_list=[]\n",
    "stopword_list=[]\n",
    "for w in words:\n",
    "    if w not in stop_word:\n",
    "        without_stopwords_list.append(w)\n",
    "    else:\n",
    "        stopword_list.append(w)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6f860bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pakistan', 'country', '.', 'I', 'love', 'pakistan']\n"
     ]
    }
   ],
   "source": [
    "print(without_stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa73d50",
   "metadata": {},
   "source": [
    "Import and Generate WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b180509c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a82ac209",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent='Pakistan is my country. I love pakistan. Pakistan is beautiful country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d9aa82b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wordcloud.wordcloud.WordCloud at 0x195a0927190>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cloud=WordCloud().generate(sent)\n",
    "cloud"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
